Distributional Tests
=================================================
Column{.tabset}
--------------------------------------------------
### Kolmogorov-Smirnov test
The Kolmogorov-Smirnov test is defined by:

H$_0$:     The data follow a specified distribution
H$_1$:     The data do not follow the specified distribution

Test Statistic:     The Kolmogorov-Smirnov test statistic is defined as

where F is the theoretical cumulative distribution of the distribution being tested which must be a continuous distribution (i.e., no discrete distributions such as the binomial or Poisson), and it must be fully specified

### Characteristics and Limitations of the K-S Test


An attractive feature of this test is that the distribution of the K-S test statistic itself does not depend on the underlying cumulative distribution function being tested. Another advantage is that it is an exact test (the chi-square goodness-of-fit test depends on an adequate sample size for the approximations to be valid). Despite these advantages, the K-S test has several important limitations:
<p>

1. It only applies to continuous distributions.
2. It tends to be more sensitive near the center of the distribution than at the tails.
3. Perhaps the most serious limitation is that the distribution must be fully specified. That is, if location, scale, and shape parameters are estimated from the data, the critical region of the K-S test is no longer valid. It typically must be determined by simulation.

<p>

Due to limitations 2 and 3 above, many analysts prefer to use the Anderson-Darling goodness-of-fit test.

However, the Anderson-Darling test is only available for a few specific distributions.



### The Anderson -Darling test

The Anderson -Darling test is a statistical test of whether there is evidence that a given sample of data did not arise from a given probability distribution.

In its basic form, the test assumes that there are no parameters to be estimated in the distribution being tested, in which case the test and its set of critical values is distribution-free. However, the test is most often used in contexts where a family of distributions is being tested, in which case the parameters of that family need to be estimated and account must be taken of this in adjusting either the test-statistic or its critical values.

When applied to testing if a normal distribution adequately describes a set of data, it is one of the most powerful statistical tools for detecting most departures from normality.




-------------------------------------------------------------
### The Shapiro-Wilk test of normality 
Performs the Shapiro-Wilk test of normality.

<pre><code>
> x<- rnorm(100, mean = 5, sd = 3)
> shapiro.test(x)

        Shapiro-Wilk normality test

data:  rnorm(100, mean = 5, sd = 3)
W = 0.9818, p-value = 0.1834
</code></pre>
<p>
In this case, the p-value is greater than 0.05, so we fail to reject the null hypothesis that the
data set is normally distributed.

<pre><code>
>y <- runif(100, min = 2, max = 4)
> shapiro.test(y)

        Shapiro-Wilk normality test

data:  runif(100, min = 2, max = 4)
W = 0.9499, p-value = 0.0008215
</code></pre>
<p>
In this case, the p-value is less than 0.05, so we reject the null hypothesis that the
data set is normally distributed.
