Here’s your content on the **Kolmogorov-Smirnov (K-S) test** cleaned up, restructured, and formatted into tidy Markdown for clarity:

---

# Kolmogorov-Smirnov Test (K-S Test)

## Purpose

The K-S test evaluates whether:

- A single sample follows a specified **continuous distribution**  
- **Two samples** come from the **same distribution**

---

## Hypotheses

### One-Sample K-S Test

- \( H_0 \): The data follow a specified distribution  
- \( H_1 \): The data do not follow the specified distribution  

> *Note: The distribution must be continuous and fully specified (i.e., known parameters).*

### Two-Sample K-S Test

- \( H_0 \): The two data sets are from the same distribution  
- \( H_1 \): The two data sets are from different distributions  

---

## Test Statistic

- Denoted as \( D \): the **maximum absolute difference** between the cumulative distribution functions (CDFs) of the observed and expected distributions.
- In the two-sample case, it compares the empirical CDFs of both samples.

---

## Characteristics of the K-S Test

✅ **Advantages:**

- The distribution of the test statistic \( D \) **does not depend** on the underlying cumulative distribution.
- It is an **exact test** (unlike chi-square, which requires large sample sizes).

⚠️ **Limitations:**

1. Only applicable to **continuous distributions**.
2. More sensitive near the **center** than in the **tails**.
3. Requires the distribution to be **fully specified**. If parameters (mean, variance) are **estimated from the data**, the usual critical values are not valid; simulations may be needed.

> Because of limitations 2 and 3, analysts often prefer the **Anderson-Darling test**, which gives more weight to the tails—but it’s only available for select distributions.

---

## R Examples

### Example 1: Similar Normal Distributions (Same Parameters)

```r
X <- rnorm(16, mean=20, sd=5)
Y <- rnorm(18, mean=21, sd=4)
ks.test(X, Y)
```

**Output:**
```
Two-sample Kolmogorov-Smirnov test
D = 0.2153, p-value = 0.7348
alternative hypothesis: two-sided
```

→ No evidence to reject the null hypothesis; distributions may be the same.

---

### Example 2: Same Distribution Type, Different Parameters

```r
X <- rnorm(16, mean=20, sd=5)
Z <- rnorm(16, mean=14, sd=5)
ks.test(X, Z)
```

**Output:**
```
Two-sample Kolmogorov-Smirnov test
D = 0.5625, p-value = 0.0112
alternative hypothesis: two-sided
```

→ Reject the null hypothesis; samples likely from different distributions despite both being normal.

---

Let me know if you’d like this turned into a revision handout, interactive quiz, or summary chart.

### Kolmogorov-Smirnov test
The Kolmogorov-Smirnov test is defined by:

H$_0$:     The data follow a specified distribution
H$_1$:     The data do not follow the specified distribution

Test Statistic:     The Kolmogorov-Smirnov test statistic is defined as

where F is the theoretical cumulative distribution of the distribution being tested which must be a continuous distribution (i.e., no discrete distributions such as the binomial or Poisson), and it must be fully specified

####  Characteristics and Limitations of the K-S Test}


An attractive feature of this test is that the distribution of the K-S test statistic itself does not depend on the underlying cumulative distribution function being tested. Another advantage is that it is an exact test (the chi-square goodness-of-fit test depends on an adequate sample size for the approximations to be valid). Despite these advantages, the K-S test has several important limitations:

* It only applies to continuous distributions.
* It tends to be more sensitive near the center of the distribution than at the tails.
*  Perhaps the most serious limitation is that the distribution must be fully specified. That is, if location, scale, and shape parameters are estimated from the data, the critical region of the K-S test is no longer valid. It typically must be determined by simulation.
<p>
Due to limitations 2 and 3 above, many analysts prefer to use the Anderson-Darling goodness-of-fit test.

However, the Anderson-Darling test is only available for a few specific distributions.

####  {Kolmogorov - Smirnov Test}

For a single sample ofdata, the Kolmogorov-Smirnov test is used to test whether or not the sample of data is consistent with a specified distribution function. (Not part of this course)
When there are two samples of data, it is used to test whether or not these two samples may reasonably be assumed to come from the same distribution.
The null and alternative hypotheses are as follows:

H0: The two data sets are from the same distribution
H1: The data sets are not from the same distribution

Consider two sample data sets X and Y that are bothnormally distributed with similar means and variances.

<pre><code>
> X=rnorm(16,mean=20,sd=5)
> Y=rnorm(18,mean=21,sd=4)
> ks.test(X,Y)

        Two-sample Kolmogorov-Smirnov test

data:  X and Y
D = 0.2153, p-value = 0.7348
alternative hypothesis: two-sided
</code></pre>
<p>
Remark: It doesn’t not suffice that both datasets are from the same distribution. They must have the same value for the defining parameters. Consider the case of data sets; X and Z. Both are normally distributed, but with different mean values.

<pre><code>
> X=rnorm(16,mean=20,sd=5)
> Z=rnorm(16,mean=14,sd=5)
> ks.test(X,Z)

        Two-sample Kolmogorov-Smirnov test

data:  X and Z
D = 0.5625, p-value = 0.0112
alternative hypothesis: two-sided
</code></pre>
<p>


--------------------------------------------------------------------------------------




\section{Kolmogorov-Smirnov test}
 The Kolmogorov-Smirnov test is defined by:
\\
H$_0$:     The data follow a specified distribution\\
H$_1$:     The data do not follow the specified distribution\\

Test Statistic:     The Kolmogorov-Smirnov test statistic is defined as

where F is the theoretical cumulative distribution of the distribution being tested which must be a continuous distribution (i.e., no discrete distributions such as the binomial or Poisson), and it must be fully specified

####  { Characteristics and Limitations of the K-S Test}


An attractive feature of this test is that the distribution of the K-S test statistic itself does not depend on the underlying cumulative distribution function being tested. Another advantage is that it is an exact test (the chi-square goodness-of-fit test depends on an adequate sample size for the approximations to be valid). Despite these advantages, the K-S test has several important limitations:
\begin{enumerate}
\item It only applies to continuous distributions.
\item It tends to be more sensitive near the center of the distribution than at the tails.
\item Perhaps the most serious limitation is that the distribution must be fully specified. That is, if location, scale, and shape parameters are estimated from the data, the critical region of the K-S test is no longer valid. It typically must be determined by simulation.
\end{enumerate}
Due to limitations 2 and 3 above, many analysts prefer to use the Anderson-Darling goodness-of-fit test.

However, the Anderson-Darling test is only available for a few specific distributions.


------------------------------------------------------------

### {Kolmogorov-Smirnov test}

The Kolmogorov-Smirnov test is defined by:

H$_0$:     The data follow a specified distribution\\
H$_1$:     The data do not follow the specified distribution\\

Test Statistic:     The Kolmogorov-Smirnov test statistic is defined as

where F is the theoretical cumulative distribution of the distribution being tested which must be a continuous distribution (i.e., no discrete distributions such as the binomial or Poisson), and it must be fully specified

####  { Characteristics and Limitations of the K-S Test}


An attractive feature of this test is that the distribution of the K-S test statistic itself does not depend on the underlying cumulative distribution function being tested. Another advantage is that it is an exact test (the chi-square goodness-of-fit test depends on an adequate sample size for the approximations to be valid). Despite these advantages, the K-S test has several important limitations:
\begin{enumerate}
\item It only applies to continuous distributions.
\item It tends to be more sensitive near the center of the distribution than at the tails.
\item Perhaps the most serious limitation is that the distribution must be fully specified. That is, if location, scale, and shape parameters are estimated from the data, the critical region of the K-S test is no longer valid. It typically must be determined by simulation.
\end{enumerate}
Due to limitations 2 and 3 above, many analysts prefer to use the Anderson-Darling goodness-of-fit test.

However, the Anderson-Darling test is only available for a few specific distributions.
