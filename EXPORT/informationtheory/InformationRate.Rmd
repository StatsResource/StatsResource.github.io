Information Rate
==========================






### Information Rate
If the time rate at which source X emits symbols is $r$ (symbols/second), the information rate R of the
source is given by

\[R = rH(X) \mbox{      (b/second)} \]

<p>

#### Information Rate : Example


*  A high-resolution TV picture consists of about $2 \times 10^6$ picture elements (symbols) and 16
different brightness levels. *  Pictures are repeated at a rate of 32 per second. *  All picture elements
are assumed to be independent, and all levels have equal likelihood of occurrence. *  Calculate the
average rate of information conveyed by this TV picture source.


<p>



#### Solution

\[R = rH(X)\] 


    *  $R$  - average rate of information
    *  $r$ - number of symbols symbols that are being transmitted per second
    *  $H(X)$ - Entropy




*  Entropy \[H(X) = - \sum \limits^{16}_{i=1} {1\over 16} \mbox{log}_2 \left( {1\over 16} \right)\] \bigskip

\[H(X) = \left[ -{1\over 16} \mbox{log}_2 \left({ 1\over 16}  \right)\right] + \left[- {1\over 16} \mbox{log}_2 \left({ 1\over 16}  \right)\right] \ldots\left[ -{1\over 16} \mbox{log}_2 \left({ 1\over 16}  \right)\right] \] 

<br><br>

*  Sixteen identical terms. Compute one and multiply by 16.

\[ H(X) = 16 \times \left[ -{1\over 16} \mbox{log}_2 \left({ 1\over 16}  \right)\right]  = -\mbox{log}_2 \left({ 1\over 16}  \right)= -(-4) = 4\] 
*  $H(X)= 4$ b
*  $r =  2(10^6)(32) = 64(10^6)$ elements/sec <br><br>



*  $R = rH(X) = 64(10^6)(4) = 256 \times 10^6) \mbox{ b/sec } = 256 \mbox{ Mb/sec }$ 

<p>

### Self Information}Self-information
This is defined by the following mathematical formula:$I(A) = −logb P(A)$

The self-information of an event measures the amount of one’s surprise
evoked by the event. The negative logarithm $−logb P(A)$ can also be written as \[
log_b  {1 \over P(A)} \]
Note that log(1) = 0, and that $| − log(P(A))|$ increases as P(A) decreases
from 1 to 0. This supports our intuition from daily experience. For example,
a low-probability event tends to cause more ``surprise".
<p>





### Code efficiency and Code redundancy
% Pg 253/254
The parameter $L$ represents the average number of bits per source symbol used in the source coding process.
The code efficiency is defined as \[\nu = {L_{min} \over L} \]where $L_{min}$ is the minimum possiblve value of $L$. When $\nu$ approaches unity, the codes is said to be efficient. 
The code redundancy $\gamma$ is defined as $\gamma = 1- \nu$.



<p>

### Source Coding Theorem}
The source coding theorem states that for zi DMS X with entropy $H(X)$, the average code word length $L$ per symbol is bounded as
L 2 H(X) (10.52)

and further, L can bc made as close to H(X) as dcsircd for some suitably chosen code.
Thus, with$ L_{min} \geq H(X)$.

The code efficiency can be rewritten as
\[\nu = {H(X) \over L} \]

<p>


