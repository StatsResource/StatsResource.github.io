More on Chi Square Tests {data-navmenu="Chi Square Tests"}
===========================================================

Column {.tabset}
-----------------------------------

### Chi Square Tests

<h3>Chi-squared Test</h3>



A $\chi^2$ test is carried out on tabular data containing counts, e.g. the
number of animals that died, the number of days of rain, the
number of stocks that grew in value, etc.
<p>
Usually have two qualitative variables, each with a number of
levels, and want to determine if there is a relationship between the
two variables, e.g. hair colour and eye colour, social status and
crime rates, house price and house size, gender and left/right
handedness.
</p><p>
The data are presented in a contingency table:
right-handed left-handed TOTAL
</p><p>
\begin{tabular}{|c|c|c|c|}
	\hline
	% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
	&amp; right-handed &amp;left-handed &amp; TOTAL\\\hline
	Male &amp; 43 &amp; 9 &amp; 52 \\
	Female &amp; 44 &amp; 4 &amp; 48 \\
	TOTAL &amp; 87 &amp; 13 &amp; 100 \\
	\hline
\end{tabular}
</p><p>

The hypothesis to be tested is:<br>
* $H0 :$There is no relationship between gender and left/right-handedness<br>
* $H1 :$There is a relationship between gender and left/right-handedness<br>
</p><p>
The values that we collect from our sample are called the observed
(O) frequencies (counts). Now need to calculate the expected (E)
frequencies, i.e. the values we would expect to see in the table, if
H0 was true.
</p>


<pre><code>
\section{Section 3. Chi-squared Test of Independence}
Two random variables x and y are called independent if the probability distribution of one variable is not affected by the presence of another.

Assume $f_{ij}$ is the observed frequency count of events belonging to both i-th category of x and j-th category of y. 
Also assume $e_{ij}$ to be the corresponding expected count if x and y are independent. 

The null hypothesis of the independence assumption is to be rejected if the p-value of the following Chi-squared test statistics is less than a given significance level $\alpha$.

<h4>Example</h4>
In the embedded data set “survey”, the Smoke column records the students smoking habit, while the Exer column records their exercise level. 
<ul>
<li> The allowed values in Smoke are "Heavy", "Regul" (regularly), "Occas" (occasionally) and "Never". 
<li> As for Exer, they are "Freq" (frequently), "Some" and "None".
</ul>

We can tally the students smoking habit against the exercise level with the table function in \texttt{R}. 

The result is called the contingency table of the two variables.
<pre>
<code>
> library(MASS)       # load the MASS package 
> tbl = table(survey$Smoke, survey$Exer) 
> tbl                 # the contingency table 
</code>
</pre>
<pre>
<code>
        Freq None Some 
  Heavy    7    1    3 
  Never   87   18   84 
  Occas   12    3    4 
  Regul    9    1    7

help(normal)
</code>
</pre>
<p>
</code></pre>


\subsection{Problem}
Test the hypothesis whether the students smoking habit is independent of their exercise level at 0.05 significance level.
Solution
We apply the chisq.test function to the contingency table tbl, and found the p-value to be 0.4828.
<pre><code>







</code></pre>


<pre><code>
> chisq.test(tbl) 
 
        Pearson’s Chi-squared test 
 
data:  table(survey$Smoke, survey$Exer) 
X-squared = 5.4885, df = 6, p-value = 0.4828 
 
Warning message: 
In chisq.test(table(survey$Smoke, survey$Exer)) : 
  Chi-squared approximation may be incorrect

</code></pre>


<pre><code>
### Answer

As the p-value 0.4828 is greater than the 0.05 significance level, we do not reject the null hypothesis that the smoking habit is independent of the exercise level of the students.

</code></pre>


<pre><code>
### Enhanced Solution
The warning message found in the solution above is due to the small cell values in the contingency table. To avoid such warning, we combine the second and third columns of tbl, and save it in a new table named ctbl. Then we apply the chisq.test function against ctblinstead.

</code></pre>


<pre><code>
ctbl = cbind(tbl[,"Freq"], tbl[,"None"] + tbl[,"Some"]) 
ctbl 
      [,1] [,2] 
Heavy    7    4 
Never   87  102 
Occas   12    7 
Regul    9    8 
</code>
</pre>



</code></pre>


<pre><code>
chisq.test(ctbl) 

</code></pre>

The hypothesis to be tested is

* $H0 :$There is no relationship between gender and left/right-handedness
* $H1 :$There is a relationship between gender and left/right-handedness

The values that we collect from our sample are called the observed (O) frequencies (counts). Now need to calculate the expected (E)
frequencies, i.e. the values we would expect to see in the table, if H0 was true.


### Chi-squared Test of Independence
Two random variables x and y are called independent if the probability distribution of one variable is not affected by the presence of another.

Assume $f_{ij}$ is the observed frequency count of events belonging to both i-th category of x and j-th category of y. Also assume eij to be the corresponding expected count if x and yare independent. 

The null hypothesis of the independence assumption is to be rejected if the p-value of the following Chi-squared test statistics is less than a given significance level α.

 
### Example
In the embedded data set “survey”, the Smoke column records the students smoking habit, while the Exer column records their exercise level. The allowed values in Smoke are "Heavy", "Regul" (regularly), "Occas" (occasionally) and "Never". As for Exer, they are "Freq" (frequently), "Some" and "None".

We can tally the students smoking habit against the exercise level with the table function in R. The result is called the contingency table of the two variables.
 


<pre><code>
library(MASS)       # load the MASS package 
tbl = table(survey$Smoke, survey$Exer) 
tbl                 # the contingency table 

</code></pre>


           
            Freq None Some
      Heavy    7    1    3
      Never   87   18   84
      Occas   12    3    4
      Regul    9    1    7



<pre><code>
Problem
Test the hypothesis whether the students smoking habit is independent of their exercise level at 0.05 significance level.
Solution
We apply the chisq.test function to the contingency table tbl, and found the p-value to be 0.4828.

</code></pre>


<pre><code>
> chisq.test(tbl) 
 
        Pearson’s Chi-squared test 
 
data:  table(survey$Smoke, survey$Exer) 
X-squared = 5.4885, df = 6, p-value = 0.4828 
 
Warning message: 
In chisq.test(table(survey$Smoke, survey$Exer)) : 
  Chi-squared approximation may be incorrect
Answer
As the p-value 0.4828 is greater than the .05 significance level, we do not reject the null hypothesis that the smoking habit is independent of the exercise level of the students.
Enhanced Solution
The warning message found in the solution above is due to the small cell values in the contingency table. To avoid such warning, we combine the second and third columns of tbl, and save it in a new table named ctbl. Then we apply the chisq.test function against ctblinstead.
 


</code></pre>


<pre><code>
ctbl <- cbind(tbl[,"Freq"], tbl[,"None"] + tbl[,"Some"]) 
ctbl 
</code></pre>


<pre><code>
chisq.test(ctbl) 
</code></pre>



#### {Chi-Square Test of Association}

\textbf{Cell$_{(1,1)}$}
\[
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline & Cat X & Cat Y & Cat Z & Total  \\ \hline
Cat A & \alert{\ldots}& 60 &  & 200\\ \hline
Cat B & \phantom{space}& \phantom{space} & \phantom{space} & 400 \\ \hline
Total & 150 & 180 & 270 &  \textbf{600}\\ \hline

\end{tabular} 
\end{center}
\]

----------------------------------------


#### {Chi-Square Test of Association}

\textbf{Cell$_{(1,1)}$}

*  Row 1 : Row Total = 200
*  Column 1 : Column Total = 150
*  Overall Total = 600




Expected value for Cell$_{(1,1)}$

\[ E_{(1,1)} = \frac{200 \times 150}{600} = \frac{30,000}{600} = 50 \]

----------------------------------------

### {Chi-Square Test of Association}
% Writing


Expected values for all of the other cells can be computed the same way.

\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline & Cat X & Cat Y & Cat Z & Total  \\ \hline
Cat A & 50 & 60 &  & 200\\ \hline
Cat B & \phantom{space}& \phantom{space} & \phantom{space} & 400 \\ \hline
Total & 150 & 180 & 270 &  \textbf{600}\\ \hline

\end{tabular} 
\end{center}





### Critical Value


*  Signifiance Level $\alpha$
*  Degrees of freedom $\nu$ (also referred to as $d.f.$)



----------------------------------------

#### Chi-Square Test of Association


The degrees of freedom $\nu$

\[ \nu  = (r-1) \times (c-1) \]



*  $r$ number of rows in frequency table
*  $c$ number of columns in frequency table




### Goodness of Fit

#### {Chi-Square Goodness of Fit}


30 Observations are thought to be indepedent realizations of a Poisson Random Variable $X$ with mean 6.


*  $X \leq 4$ 10 occurrences
*  $4 < X \leq 7$ 8 occurrences
*  $ X >7$ 12

Test the hypothesis that the observations on $X$ are from a Poisson Distribution with mean 6.

----------------------------------------

Contingency tables are used to examine the relationship between subjects' scores on two qualitative or categorical variables. 




#### {Example}

*  Consider the hypothetical experiment on the effectiveness of early childhood intervention programs described in another section.
*  In the experimental group, 73 of 85 students graduated from high school. In the control group, only 43 of 82 students graduated. These data are depicted in the contingency table shown below.



----------------------------------------------------------------- %

#### {Chi Square test for goodness of fit}
The chi-squared test applied to contingency tables.

The Chi-squared test is the most commonly used test for frequency data and goodness-of-fit. In theory, it is nonparametric but because it has no parametric equivalent, it is not classified as such. It is not an exact test and with the current level of computing facilities, there is not much excuse not to use Fishers exact test for $2\times2$ contingency table analysis instead of Chi-squared test. 

-----------------------------------------





#### {Chi-Square Test of Association}


Expected Value for a Cell

\[ = \frac{\mbox{Column Total}  \times \mbox{Row Total} } {\mbox{Overall Total}}  \]


<p>


#### {Chi-Square Test of Association}




 
Chi-Square Tests for independence.
 
Contingency tables are used to examine the relationship between scores on two qualitative or categorical variables for the subjects of a study. Statistical procedures known as Chi-Square Tests for independence are used to test the relationship between these variables.
 
For example, consider the hypothetical experiment on the effectiveness of early childhood intervention programs.
 
 
<p>


#### {Chi-Square Test of Association}



In the experimental group, 73 of 85 students graduated from high school. In the control group, only 43 of 82 students graduated. These data are depicted in the contingency table shown below. 
 
 
 
Graduated
Failed to
Graduate
Total
Experimental
73
12
85
Control
43
39
82
Total
116
51
167
The cell entries are cell frequencies. The top left cell with a "73" in it means that 73 subjects in the experimental condition went on to graduate from high school; 12 subjects in the experimental condition did not.
 
An inspection of the contingency table shows that subjects in the experimental condition were more likely to graduate than were subjects in the control condition. Thus, the column a subject is in (graduated or failed to graduate) is contingent upon (depends on) the row the subject is in (experimental or control condition). 
 
 
<p>


#### {Chi-Square Test of Association}


Null hypothesis
 
The test of whether the columns are contingent on the rows is called the chi square test of independence. The null hypothesis is that there is no relationship between row and column frequencies.
 
For this example, this is equivalent to saying that the intervention program has no effects on a students performance ; students from both groups are equally as likely to graduate, and that the program does not improve the chance of graduating.
 
If the columns are not contingent on the rows, then the rows and column frequencies are independent. If this was the case, on average an equal proportion would graduate and fail fron both experimental and control groups. Under this hypothesis, any differences between the observation and the expected values can be attributed to the random sampling effects.
 
 
%---------------------------------------------%


#### {Chi-Square Test of Association}



Alternative hypothesis
 
As always, the alternative hypothesis is the opposite of the null hypothesis, that there is a relationship between row and column frequencies.
This is equivalent to saying that the intervention program does have an effect on a students performance; that the program does indeed improve the chance of graduating. 
 
A statistical procedure is carried out to determine the p-value associated with this test. A decision to reject the null hypothesis is based on this p-value.
 
 
%---------------------------------------------%


#### {Chi-Square Test of Association}



Expected values
 
The first step in computing the chi square test of independence is to compute the expected frequency for each cell under the assumption that the null hypothesis is true. To calculate the expected frequency of the first cell in the example (experimental condition, graduated), first calculate the proportion of subjects that graduated without considering the condition they were in. The table below shows that of the 167 subjects in the experiment, 116 graduated.
 
Therefore, 116/167 graduated. If the null hypothesis were true, the expected frequency for the first cell would equal the product of the number of people in the experimental condition (85) and the proportion of people graduating (116/167). This is equal to (85)(116)/167 = 59.042. Therefore, the expected frequency for this cell is 59.042. The general formula for expected cell frequencies is:
 
  
where
 
Eij is the expected frequency for the cell in the ith row and the jth column
Tiis the total number of subjects in the ith row
Tj is the total number of subjects in the jth column
N is the total number of subjects in the whole table. 

%---------------------------------------------%


#### {Chi-Square Test of Association}


 
Once the expected cell frequencies are computed, it is convenient to enter them into the original table as shown below. The expected frequencies are in brackets. 
 
 
 
Graduated
Failed to
Graduate
Total
Experimental
73
(59.042)
12
(25.958)
85
Control
43
(56.958)
 
39
(25.042)
82
Total
116
51
167
The formula for chi square test for independence is 
 
 
%---------------------------------------------%


#### {Chi-Square Test of Association}


 
Degrees of Freedom 
The degrees of freedom are equal to (R-1)(C-1) where R is the number of rows and C is the number of columns. In this example, R = 2 and C = 2, so df = (2-1)(2-1) = 1.
 
The chi square test statistic is compared to a critical value of the chi-square distribution with degrees of freedom (R-1)(C-1). This is a one tailed test. The null hypothesis is rejected if the test statistic is greater than the critical value. Alternatively a p-value will be computed and compared to a pre-specified significance level.
 
 
%---------------------------------------------%


#### {Chi-Square Test of Association}


A computational procedure will determine that for df = 1, a chi square test statistic of 22.01 has a probability value less than 0.0001.
 
 
The results for the example on the effects of the early childhood intervention example could be reported as follows:
The proportion of students from the early-intervention group who graduated from high school was 0.86 whereas the proportion from the control group who graduated was only 0.52. The difference in proportions is significant, χ²(1, N = 167) = 22.01, p < 0.001.
 
 
%---------------------------------------------%


#### {Chi-Square Test of Association}


Example 2 
 
The same procedures are used for analyses with more than two rows and/or more than two columns. For example, consider the following hypothetical experiment: A drug that decreases anxiety is given to one group of subjects before they attempted to play a game of chess against a computer. The control group was given a placebo. The contingency table is shown below, with expected values in brackets.
 
Condition
Win
Lose
Draw
Total
Drug
12
(14.29)
18
(14.29)
10
(11.43)
40
Placebo
13
(10.71)
7
(10.71)
10
(8.57)
30
Total
25
25
20
70
 
As in the previous example, each expected frequency is computed by multiplying the row total by the column total and dividing by the total number of subjects.
 
%---------------------------------------------%


#### {Chi-Square Test of Association}


For example, the expected frequency for the "Drug-Lose" condition is the product of the row total (40) and the column total (25) divided by the total number of subjects (70): (40)(25)/70 = 14.29. 
 
 
%---------------------------------------------%


#### {Chi-Square Test of Association}


The chi square is calculated using the formula:
 
 
 
 
 
The df are (R-1)(C-1) = (2-1)(3-1) = 2.

 
<p>


A chi square table shows that the probability of a chi square of 3.52 with 2 degrees of freedom is 0.172. Therefore, the effect of the drug is not significant. 
 
 The experiment on the effect of the anti-anxiety drug on chess playing could be reported as:
The number of subjects winning,losing,and drawing as a function of drug condition is shown in figure below. Although subjects receiving the drug performed slightly worse than subjects not receiving the drug, the difference was not significant, χ²(2, N = 70) = 3.52, p = 0.17. 
 
 
<p>


Comments on the Chi-square test 
 
The formula for chi square yields a statistic that is only approximately a chi square distribution. In order for the approximation to be adequate, the total number of subjects should be at least 20.
 
Some authors claim that the correction for continuity should be used whenever an expected cell frequency is below 5. Research in statistics has shown that this practice is not advisable.


### Other Important tests: 
 
1) Hypothesis test for regression coefficients and correlation.
 
For all types of correlation, there is an associated hypothesis test to examine whether or not the true value of a parameter is zero. This is particularly important in the case of the slope. If the true value is zero, this means that no relationship exists between the independent variable and dependent variable.
 
    
 
In several scientific analyses (e..g calibration studies) , it is important to test whether the true value of the intercept. The test is formulated as follows. 
 
    
 
Necessarily the test for pearon's correlation is the equivalent of the test of the slope. The null hypothesis is that the true value is zero, against the alternative hypothesis which states that it is not zero.
 
    
 
The decision to reject or fail to reject is based on the p-value. 
 
 
2)  F-Test for Equality of Two Standard Deviations.
 
An F-test is used to test if the standard deviations of two populations are equal. 
The two-tailed version tests against the alternative that the standard deviations are not equal. The one-tailed version only tests in one direction, that is the standard deviation from the first population is either greater than (or less than ) the second population standard deviation .
 
 
 
Levene's test is used to test if multiple samples have equal variances. Equal variances across samples is called homogeneity of variance.
 

 
Ha:ij  for some i and j
 
The simple two-sample test for means  is based on the assumption of equal variance of both samples. If the variances are not the same, the analysis for the two-sample must vary accordingly.
 
 
3) Tests for distributions
 
The Kolmorgorov-Smirnov test and the Anderson-Darling test are formulated as follows:
 
         Ho:  The data follows a specified distribution.  
         Ha:  The data do not follow the specified distribution 
 
These tests are one-sided test and the hypothesis that the distribution is of a specific form is rejected if the test statistic is greater than the critical value. They are commonly used to assess whether a data set is normally distributed.
 
The Shapiro-Wilk test is another procedure which specifically tests for the normal distribution
 
         Ho:  The data follows the normal distribution.  
         Ha:  The data do not follow the normal distribution 





### Tests for Goodness of Fit

*  Basic Counting Model
*  Goodness of Fit Statistics
*  Testing with unknown parameters
*  Testing for association in two-way tables


#### {Pearson's chi-squared test}

Chi Square

\begin{description}
* [Null Hypothesis]
There is no relationship between the two categorical variables

* [Alternative Hypothesis]
There is a relationship between the two categorical variables
\end{description}


*  Observed Values

*  Expected Values (under the null hypothesis)



Are the differences between Observed values and the Expected values
small enough to be due to random error (i.e. null hypothesis is valid)
or too large for the null hypothesis to be feasible?


-----------------------------------------------------
<p>

Expected values for each cell

Row Total Column Total
Overall Total

E-O/E


-----------------------------------------------------
<p>

#### {Degrees of freedom}

\[df=(r-1)(c-1)\]


* [r] = number of rows
* [c] = number of columns


2 rows and 3 columns r = 2 c = 3 
\[df= (2-1)(3-1) = (1)(2) = 2\]

-----------------------------------------------------
<p>

Pearson's chi-squared test uses a measure of goodness of fit which is the sum of differences between observed and expected outcome frequencies (that is, counts of observations), each squared and divided by the expectation:
\[ \chi^2 = \sum_{i=1}^n {\frac{(O_i - E_i)}{E_i}^2} \]
where:


* [Oi] = an observed frequency (i.e. count) for bin i
* [Ei] = an expected (theoretical) frequency for bin i, asserted by the null hypothesis.


-----------------------------------------------------
<p>


Also for larger contingency tables, the G-test (log-likelihood ratio test) may be a better choice. The Chi-square value is obtained by summing up the values (residual2/fit) for each cell in a contingency. In this formula, residual is the difference between the observed value and its expected counterpart and fit is the expected value.


--------------------------------------------------------------------------------------


\noindent \textbf{Yates's correction}
The approximation of the Chi-square statistic in small $2\times2$ tables can be improved by reducing the absolute value of differences between expected and observed frequencies by 0.5 before squaring. This correction, which makes the estimation more conservative, is usually applied when the table contains only small observed frequencies ($<20$).

The effect of this correction is to bring the distribution based on discontinuous frequencies nearer to the continuous Chi-squared distribution. This correction is best suited to the contingency tables with fixed marginal totals. Its use in other types of contingency tables (for independence and homogeneity) results in very conservative significance probabilities. This correction is no longer needed since exact tests are available.


-----------------------------------------------------
<p>

#### {Chi-Square Test of Association}


Expected Value for a Cell

\[ = \frac{\mbox{Column Total}  \times \mbox{Row Total} } {\mbox{Overall Total}}  \]



*  Compute the expected values for each cell in the following table.
*  One of the expected values (both A and Y) is given.

\begin{center}

\begin{tabular}{|c|c|c|c|c|}
\hline
 & Cat X & Cat Y & Cat Z & Total  \\ \hline
Cat A & & 60 &  & 200\\ \hline
Cat B & &  &  & 400 \\ \hline
Total & 150 & 180 & 270 &  \textbf{600}\\ \hline
\end{tabular} 
\end{center}




------------------------------------------


#### {Chi-Square Test of Association}

\textbf{Cell$_{(2,1)}$}\

*  Row 2 : Row Total = 400
*  Column 1 : Column Total = 150
*  Overall Total = 600


Expected value for Cell$_{(2,1)}$

\[ E_{(2,1)} = frac{400 \times 150}{600} = \frac{60,000}{600} = 100 \]

-----------------------------------------------------
<p>


\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline 
 & Cat X & Cat Y & Cat Z & Total  \\ \hline
Cat A & 50 & 60 &  & 200\\ \hline
Cat B & \phantom{s}100\phantom{s}& \phantom{space} & \phantom{space} & 400 \\ \hline
Total & 150 & 180 & 270 &  \textbf{600}\\ \hline
\end{tabular} 
\end{center}


-----------------------------------------------------
<p>


\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline 
 & Cat X & Cat Y & Cat Z & Total  \\ \hline
Cat A & 50 & 60 & 90  & 200\\ \hline
Cat B & \phantom{s}100\phantom{s}& \phantom{s}120\phantom{s} & \phantom{s}180\phantom{s} & 400 \\ \hline
Total & 150 & 180 & 270 &  \textbf{600}\\ \hline
\end{tabular} 
\end{center}

-----------------------------------------------------
<p>
