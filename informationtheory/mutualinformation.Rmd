


###  MUTUAL INFORMATION


#### Entropy

\item Entropy is the uncertainty of a single random variable. 
\item We can define \textbf{\emph{conditional entropy }}$H(X|Y)$, which is the entropy of a random variable
conditional on the knowledge of another random variable. 
\item The reduction in uncertainty due to another random variable is called the \textbf{\emph{mutual information}}.


#### A. Conditional and Joint Entropies
Using the input probabilities P(x,), output probabilities $P(y_i)$, transition probabilities P(yJ|>r,),
and joint probabilities P(x,, yy), we can define the following various entropy functions for a channel
with m inputs and n outputs:
 

*  H(X) = - X P(xi) log %; P(x;) (10.21)
*  H(Y) = -2P(yj)]%<>gz Pty;) UU-22)
*  H<X I Y) = - X X %P<>¤r,yy)1<¤gz Ptmlyj) <10.23)
*  H = -2 ZP%(>¢..y,) 1022 P(y,|X.) (10-24)
*  H(X, Y) - -2 Z F%<><..y,)1¤zz P(><r.y;) <10·25)



 \textbf{Mutual Information}
Mutual information is one of many quantities that measures how much one random variables gives about another. It is a dimensionless quantity. Mutual Informaiton can be thought of as the reduction in uncertainty about one random variable given knowledge of another. 
\begin{itemize}\item High mutual information indicates a large reduction in uncertainty, \item low mutual information indicates a small reduction, \item  zero mutual information between two random variables means the variables are independent.
\end{itemize}
Efficient communication systems have high mutual information.

%---------------------------------------------------------------------------------------------------------------------------------------%

 \textbf{Mutual Information}

\textbf{Joint Entropies:}\\
Using the input probabilities $P(x_i)$, output probabilities $P(y_i)$, transition probabilities $P(y_i|x_i)$,
and joint probabilities $P(x_i,y_j)$, we can define the following various entropy functions for a channel
with m inputs and n outputs:

\begin{itemize}
\item $H(X) = - \sum ^{m}_{i=1} P(x_i) \mbox{log}_2 P(x_i)$%; P(x;) (10.21)
\item $H(Y) = - \sum ^{n}_{j=1} P(y_j) \mbox{log}_2 P(y_j)$
\item $H(X, Y)= - \sum ^{n}_{j=1}\sum ^{m}_{i=1} P(x_i,y_j) \mbox{log}_2 P(x_i,y_j)$
\end{itemize}


%---------------------------------------------------------------------------------------------------------------------------------------%

\noindent \textbf{Mutual Information: Joint Entropy}

These entropies can be interpreted as follows:\begin{itemize}\item $ H(X)$ is the average uncertainty of the channel input,
and $H(Y)$ is the average uncertainty of the channel output.\item  The joint entropy $H(X,Y)$ is the average uncertainty of the communication channel as a
whole.\end{itemize}

%---------------------------------------------------------------------------------------------------------------------------------------%

\noindent \textbf{Mutual Information: Conditional Entropy}
\begin{itemize}
\item The conditional entropy $H(X|Y)$ is a
measure of the average uncertainty remaining about the channel input after the channel output has
been observed. \item This is sometimes called the equivocation of X with respect to Y.  \item The
conditional entropy $H(Y|X)$ is the average uncertainty of the channel output given that X was
transmitted.
\end{itemize}
\begin{itemize}
\item $H(X|Y)= - \sum ^{n}_{j=1}\sum ^{m}_{i=1} P(x_i,y_j) \mbox{log}_2 P(x_i|y_j)$
\item $H(Y|X)= - \sum ^{n}_{j=1}\sum ^{m}_{i=1} P(x_i,y_j) \mbox{log}_2 P(y_j|x_i)$
\end{itemize}

%-----------------------------------------------------------------------------------------------%

\noindent \textbf{Mutual Information : Useful Identities}
Two useful relationships among the types of entropies are
\begin{itemize}
\item $H(X,Y)=H(X|Y)+H(Y) $
\item $H(X,Y)=H(Y|X)+H(X) $
\end{itemize}
(Remark : compare to identities in probability theory)

%-----------------------------------------------------------------------------------------------%

\noindent \textbf{Mutual Information : Useful Identities}
\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{./11BMutualInfo}
\caption{}
\label{fig:11BMutualInfo}
\end{figure}








\noindent \textbf{Mutual Information}
The mutual information $I(X; Y)$ of a channel is defined by
\[ I(X; Y) = H(X) -  H(X|Y) \mbox{    (b/symbol) } \]

Alternatively we can define it as either
\[ I(X; Y) = H(Y) -  H(Y|X) \mbox{     (b/symbol) } \]
 or as
\[ I(X; Y) = H(Y)+ H(Y)  - H(X,Y) \mbox{    (b/symbol) } \]
Remark: The
mutual information is the reduction of
entropy of X when Y
is
known.


----------------------------------------------------------------------------------------------------------------------------

<p>

These entropies can be interpreted as follows: H(X) is the average uncertainty of the channel input,
and H(Y) is the average uncertainty of the channel output. 
* The conditional entropy H(X]Y) is a
measure of the average uncertainty remaining about the channel input after the channel output has
been observed. And H(X] Y) is sometimes called the equivncation of X with respect to Y.  
*  The ***conditional entropy*** $H(Y|X)$ is the average uncertainty of the channel output given that X was
transmitted.
* The ***joint entropy*** $H(X, Y)$ is the average uncertainty of the communication channel as a
whole.

------------------------------------------------------------------------------------

Two useful relationships among the above various entropies are
 * 
$H(X, Y)=H(X|Y)+H(Y) (10,26)$
$H(X,Y)=H(Y|X)+H(X) (10.27)$

#### B. Mutual Information:
The mutual information 1(X; Y) of a channel is deiined by
\[I(X; Y) = H(X)— H(X|Y) b/symbol\]



----------------------------------------------------------------------------------------------------------------------------
