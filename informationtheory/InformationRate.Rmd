


### Information Rate}
If the time rate at which source X emits symbols is r (symbols/s), the lnformation rate R of the
source is given by

%\[R ; r11(X) b/s (10.10)\]





### Information Rate
If the time rate at which source X emits symbols is $r$ (symbols/second), the information rate R of the
source is given by

\[R = rH(X) \mbox{      (b/second)} \]

<p>

#### Information Rate : Example


*  A high-resolution TV picture consists of about $2 \times 10^6$ picture elements (symbols) and 16
different brightness levels. *  Pictures are repeated at a rate of 32 per second. *  All picture elements
are assumed to be independent, and all levels have equal likelihood of occurrence. *  Calculate the
average rate of information conveyed by this TV picture source.






*  $H(X) = - \sum \limits^{16}_{i=1} \frac{1}{16} \mbox{log}_2 \frac{1}{16}$ <br><br>

*  i.e. $H(X) = [ -\frac{1}{16} \mbox{log}_2\frac{1}{16} ] + [- \frac{1}{16} \mbox{log}_2\frac{1}{16} ] \ldots [ -\frac{1}{16} \mbox{log}_2\frac{1}{16}) ] $ <br><br>
*  Sixteen identical terms. Compute one and multiply by 16.

\[ H(X) = 16 \times [ -\frac{1}{16} \mbox{log}_2\frac{1}{16} ]  = -\mbox{log}_2\frac{1}{16} = -(-4) = 4\] <br><br>
*  $H(X)= 4$ b
*  $r =  2(10^6)(32) = 64(10^6)$ elements/sec <br><br>

*  $R = rH(X) = 64(10^6)(4) = 256(l0^6) \mbox{ b/sec } = 256 \mbox{ Mb/sec }$ <br><br>




### Self Information}Self-information
This is defined by the following mathematical formula:$I(A) = −logb P(A)$

The self-information of an event measures the amount of one’s surprise
evoked by the event. The negative logarithm $−logb P(A)$ can also be written as \[
log_b  {1 \over P(A)} \]
Note that log(1) = 0, and that $| − log(P(A))|$ increases as P(A) decreases
from 1 to 0. This supports our intuition from daily experience. For example,
a low-probability event tends to cause more ``surprise".
<p>


### Code efficiency and Code redundancy}
% Pg 253/254
The parameter $L$ represents the average number of bits per source symbol used in the source coding process.
The code efficiency is defined as \[\nu = {L_{min} \over L} \]where $L_{min}$ is the minimum possiblve value of $L$. When $\nu$ approaches unity, the codes is said to be efficient. 
The code redundancy $\gamma$ is defined as $\gamma = 1- \nu$.



<p>

### Source Coding Theorem}
The source coding theorem states that for zi DMS X with entropy $H(X)$, the average code word length $L$ per symbol is bounded as
L 2 H(X) (10.52)

and further, L can bc made as close to H(X) as dcsircd for some suitably chosen code.
Thus, with$ L_{min} \geq H(X)$.

The code efficiency can be rewritten as
\[\nu = {H(X) \over L} \]

<p>


