

{Entropy}

*  The quantity $H(X)$ is called the \emph{\textbf{entropy}} of source $X$. *  It is a measure of the average information content per random symbol.
*  The source entropy $H(X)$ can be considered as the average amount of uncertainty
within source $X$ that is resolved by use of the alphabet.

*  Note that for if  binary source X that generates independent symbols $0$ and $1$ with equal probability, the source entropy $H(X)$ is
\[ H(X ) = -1/2 \mbox{log}_2 (1/2) - 1/2 \mbox{log}_2 (1/2) \mbox{   b/symbol}  \]
%(Remark :$\mbox{log}_2 ({1\over 2}) = -1$).

<p>


*  The source entropy$ H(X)$ satisfies the following relation:
\[0 \leq H(X) \leq \mbox{log}_2(m) \]where $m$ is the size (number of symbols) of the alphabet of source X ).
*   The lower bound corresponds to no uncertainty, which occurs when one symbol has probability $P(x_i) = l$ (i.e. X emits the same symbol all the time.
*  The upper bound corresponds to the maximum uncertainty which occurs when $P(x_i) = 1 /m$ for all $i$. that is, when all symbols are equally likely to be emitted by X.
 

<p>

{Entropy: Example}
A DMS $X$ has four symbols $x_1 , x_2, x_3, x_4$ with probabilities $P(x_1) = 0.4, P(x_2) = 0.3. P(x_3) = 0.2.
P(x_4) = 0.1$.

* [(a)] Calculate $H(X)$.
* [(b)] Find the amount of information contained in the messages $x_lx_2x_lx_3$ and $x_4x_3x_3x_2$.


<p>

#### Solution

\[ H(X) = - \sum \limits^{4}_{i=1} P(x_i) log_2 [P(x_i)] \]

\[ H(X) = -0.4\mbox{log}_2(0.4) - 0.3\mbox{log}_2(0.3)  -0.2\mbox{log}_2(0.2)  -0.1\mbox{log}_2(0.14) \]



\[ H(X) =  0.5288 + 0.5210 + 0.4644 + 0.3322  = #### 1.85} \mbox{b/sec} \]





*  (Remark: from probability, recall independent events) <br><br>
*  $P(x_lx_2x_lx_3) = 0.4\times 0.30 \times 0.40 \times 0.20  = 0.0096$ <br><br>
*  $I(x_lx_2x_lx_3) = -\mbox{log}_2(0.0096)  = 6.70$b/symbol <br><br>

<p>



*  $P(x_4x_3x_3x_2) = 0.1\times 0.20 \times 0.20 \times 0.30  = 0.0012$ <br><br>
*  $I(x_lx_2x_lx_3) = -\mbox{log}_2(0.0012)  = 9.70$b/symbol <br><br>


